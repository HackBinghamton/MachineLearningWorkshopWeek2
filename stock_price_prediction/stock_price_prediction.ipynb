{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Stock Price Prediction\n",
    "## Overview\n",
    "\n",
    "### What You'll Learn\n",
    "In this section, you'll learn\n",
    "1. How to format data so that it can be used for machine learning\n",
    "2. How to create, train, and test a model that predicts stock prices\n",
    "3. How to improve it\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. [Basic Python (functions, loops, lists)](https://github.com/HackBinghamton/PythonWorkshop)\n",
    "2. [scikit-learn](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/intro_ml_scikit.ipynb) ([Boston Housing Price example](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/housing_price_prediction.ipynb) if you'd like extra practice)\n",
    "\n",
    "### Introduction\n",
    "Stock price prediction has been a Holy Grail of machine learning for years. If one can predict changes in stock prices, they can buy and sell at just the right times to make tons of money. In this workshop section, we'll discuss how to make data about a given stock fit into a `sklearn` machine learning model as well as how to train and test it. \n",
    "\n",
    "---\n",
    "\n",
    "## Loading the Data\n",
    "The first step in starting a project is loading the data. Usually, you have to use one of `sklearn`'s datasets, find a third-party dataset online, or build your own.\n",
    "\n",
    "In this case, we've prepared datasets on different individual stocks for everyone! These datasets hold three types of data over multiple rows. In each row, you will find:\n",
    "\n",
    " 1. The date of the following data\n",
    " 2. The stock price change on that day\n",
    " 3. The average sentiment (attitude) of news related to the stock on that day\n",
    " \n",
    "Feel free to tweak the `dataset` and `train_proportion` to fit your needs. `dataset` signifies which dataset of our you'd like to load, while `train_proportion` indicates how much of the data you'd like to dedicate to testing as opposed to training.\n",
    "\n",
    "Once you've tweaked these values to your liking, run the code block and your dataset will be loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "# TWEAK THIS VALUE TO USE WHATEVER DATASET YOU'D LIKE\n",
    "# Options: Facebook, Amazon, Microsoft, Nvidia, Apple\n",
    "dataset = \"Amazon\"\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "## DO NOT MODIFY BELOW ##\n",
    "#########################\n",
    "\n",
    "##### LOAD DATA #####\n",
    "# Fetch the dataset contents\n",
    "r = requests.get(\"https://raw.githubusercontent.com/HackBinghamton/MachineLearningWorkshopWeek2/master/stock_price_prediction/\" + dataset + \".csv\")\n",
    "\n",
    "# Write to a local file\n",
    "with open(dataset + \".csv\", \"w\") as datafile:\n",
    "    datafile.write(r.text)\n",
    "\n",
    "# Read into rows\n",
    "data = []\n",
    "with open(dataset + \".csv\", \"r\") as datafile:\n",
    "    reader = csv.reader(datafile)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "##### DISPLAY DATA #####\n",
    "print(dataset, \"dataset:\")\n",
    "for row in data:\n",
    "    print(\"{0:12s} | {1:20s} | {2:14s}\".format(*row))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Great! So we've now loaded up a dataset into a list that looks something like this:\n",
    "\n",
    "```python\n",
    "[[ \"Date\",       \"Stock Change\", \"Sentiment\" ],\n",
    " [ \"2019-09-19\", \"27.55\",        \"16.98\"     ],\n",
    " [ \"2019-09-22\", \"-8.30\",        \"15.40\"     ],\n",
    " ...\n",
    " [ \"2019-09-29\", \"-8.92\",        \"15.06\"     ]]\n",
    "```\n",
    "\n",
    "However, we can't yet feed this into our machine learning model. Here are a few problems with it:\n",
    "\n",
    "1. The first row, `[\"Date\", \"Stock Change\", \"Sentiment\"]`, is not a valid data point\n",
    "2. The \"Date\" column is largely irrelevant, since all of the dates are within the same week. Unless we have multiple years worth of data points, this data is likely to cause confusion for our machine learning model\n",
    "3. All of the data is in string format (whether it's a number or not)\n",
    "\n",
    "In order to do fix this and make our data compatible with the machine learning model, we'll have to do the following:\n",
    "\n",
    "1. Remove the first row\n",
    "2. Remove the date column\n",
    "3. Convert all of the number data to `float`s\n",
    "4. Break our data into 4 different lists: training and testing sets of both X (input/news sentiment) and Y (output/stock price) values\n",
    "\n",
    "Let's do it!\n",
    "\n",
    "### 1. Chopping the top row off\n",
    "Removing the top row of our data with `[\"Date\", \"Stock Change\", \"Sentiment\"]` will let us avoid running into errors when we try to feed it into our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Slices\" in Python (the [:] things) let you chop out parts of lists to your liking\n",
    "data = data[1:]\n",
    "\n",
    "# Display the data\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chopping the Date column off\n",
    "The Date column is irrelevant, and is more likely to confuse the machine learning model than help it. It's all string data, and machine learning models like the ones we're using only take numerical data. Also, if there were years worth of data, maybe the model could find a correlation, but because there are only a few data points, there's no way that a model can make informed decisions off of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use slices again, but this time on each row (since we're deleting a column)\n",
    "for row in range(len(data)):\n",
    "    data[row] = data[row][1:]\n",
    "\n",
    "# Display the data\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting all numerical data to `float`\n",
    "As we said above, our models only take numerical data. If we try to feed it strings, it'll error out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each item and convert to float\n",
    "for row in range(len(data)):\n",
    "    for col in range(len(data[row])):\n",
    "        data[row][col] = float(data[row][col])\n",
    "\n",
    "# Display the data\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting into training and testing data\n",
    "For our last step in data processing, we must split our data into training and testing groups, as well as organize our data so that it fits into our model.\n",
    "\n",
    "In order to make our data fit into the model, we must also make each row within our inputs and outputs into its own list. To illustrate:\n",
    "\n",
    "**Before:**\n",
    "```python\n",
    "#   y        x\n",
    "[[27.55,  16.97],\n",
    " [-8.29,  15.40],\n",
    "       ...\n",
    " [-8.92,  15.06]]\n",
    "```\n",
    "\n",
    "**After (without being broken into training and testing):**\n",
    "```python\n",
    "#   y\n",
    "[[27.55 ],\n",
    " [-8.29 ],\n",
    "   ...\n",
    " [-8.92 ]]\n",
    "\n",
    "#   x \n",
    "[[16.97],\n",
    " [15.40],\n",
    "   ...\n",
    " [15.06]]\n",
    "```\n",
    "\n",
    "This may seem confusing, but we need to put each data point's X values and Y values into individual arrays because more often than not, there will be multiple input (X) variables that map to a certain output (Y) value. For example, the Boston Housing dataset from last week had inputs for crime rate, student-teacher ratio, avg. property tax, and more, with an output of the house price.\n",
    "\n",
    "Because many people use multiple inputs, we must follow suit and make sure that our data is in a compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate how much of our dataset we'd like to dedicate to training (the rest goes to testing)\n",
    "train_proportion = 0.7\n",
    "\n",
    "# Create arrays for each type of data\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "# Iterate through each row of data dedicated to training\n",
    "# Append a list into each training/testing list\n",
    "for row in range(int(train_proportion * len(data))):\n",
    "    ytrain.append([data[row][0]])\n",
    "    xtrain.append([data[row][1]])\n",
    "\n",
    "# Iterate through each row of data dedicated to testing\n",
    "# Append a list into each training/testing list\n",
    "for row in range(int(train_proportion * len(data)), len(data)):\n",
    "    ytest.append([float(data[row][0])])\n",
    "    xtest.append([float(data[row][1])])\n",
    "\n",
    "# Display our newly-formatted data\n",
    "print(\"X Training:\", xtrain)\n",
    "print(\"Y Training:\", ytrain)\n",
    "print(\"X Testing:\", xtest)\n",
    "print(\"Y Testing:\", ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, Training, and Testing Our Model\n",
    "\n",
    "Now that we have our data properly formatted, we can finally create our model and run the data through it.\n",
    "\n",
    "We've decided to use a Ridge regression arbitrarily -- poke around and see what other regressions you can use (LinearRegression and Lasso are a couple others), and how they affect the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab our model from sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create our model\n",
    "model = Ridge()\n",
    "\n",
    "# Train the model\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Test the model, and report its accuracy\n",
    "print(\"Accuracy:\", str(model.score(xtest, ytest) * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Could Do to Improve This System\n",
    "\n",
    "You probably noticed that the accuracy of our model is *very* low. Don't worry! This is normal -- let's talk about why.\n",
    "\n",
    "#### 1. Not enough data\n",
    "Machine learning models need as much data as they can get in order to make the most educated estimates. Our datasets contain roughly 10 days worth of stock data -- imagine how much better it would be if we had access to 10 years worth.\n",
    "\n",
    "#### 2. Not enough variables\n",
    "Trying to predict stock prices based on news sentiments is like trying to predict the weather based on the average humidity. Both stock prices and the weather are very chaotic systems -- drastic changes can occur suddenly and unpredictably. In order to get better at predicting stock prices, we need not only more data, but more *types* of data.\n",
    "\n",
    "In this workshop, we used news sentiment as one input. We could also gather data on the daily market average, the time of year, the time-proximity to nearby holidays, and so much more. The best models use the most data.\n",
    "\n",
    "## Appendix: How We Collected Our Datasets\n",
    "We used the [Alpha Vantage API](https://www.alphavantage.co/documentation/) to collect stock data on a daily basis, and the [News API](https://newsapi.org/) to gather news articles from the past month. To create average news sentiments, [we used the Natural Language Toolkit](https://www.nltk.org/) Vader analyzer.\n",
    "\n",
    "To learn how to do this kind of data collection yourself and interact with websites online, come to our Webscraping and APIs workshop next week!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
